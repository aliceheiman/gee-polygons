{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c5f8c8f",
   "metadata": {},
   "source": [
    "# collection\n",
    "\n",
    "> `SiteCollection` for batch operations on multiple sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef8b423",
   "metadata": {},
   "outputs": [],
   "source": "#| default_exp collection"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923e21af",
   "metadata": {},
   "outputs": [],
   "source": "#| hide\nfrom nbdev.showdoc import *"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed8c451",
   "metadata": {},
   "outputs": [],
   "source": "#| export\nfrom __future__ import annotations\nimport json\nfrom pathlib import Path\nfrom typing import Optional, Union, Iterator, Callable, Any, List, TYPE_CHECKING\nfrom dataclasses import dataclass, field\n\nimport ee\nimport pandas as pd\n\nfrom gee_polygons.site import Site, _detect_crs, _reproject_geometry\n\nif TYPE_CHECKING:\n    from gee_polygons.layers import CategoricalLayer, ContinuousLayer"
  },
  {
   "cell_type": "markdown",
   "id": "d5c3d6cf",
   "metadata": {},
   "source": [
    "## Chunking Utilities\n",
    "\n",
    "Internal utilities for splitting large site collections into manageable chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052fa19b",
   "metadata": {},
   "outputs": [],
   "source": "#| export\ndef calculate_chunk_size(n_years: int, n_sites: int, target_rows: int = 40_000) -> int:\n    \"\"\"Calculate optimal chunk size based on expected data volume.\n\n    Heuristics:\n    - Target ~40,000 result rows per getInfo() call (conservative for GEE)\n    - Each site-year produces ~10-50 class rows for categorical data\n    - For continuous, each site-year produces 1 row per band\n\n    Args:\n        n_years: Number of years being extracted\n        n_sites: Total number of sites\n        target_rows: Target maximum rows per chunk (default 40,000)\n\n    Returns:\n        Recommended number of sites per chunk\n    \"\"\"\n    # Conservative estimate: 30 classes per site-year for categorical\n    estimated_classes = 30\n    rows_per_site = n_years * estimated_classes\n\n    if rows_per_site == 0:\n        return min(100, n_sites)\n\n    chunk_size = max(10, target_rows // rows_per_site)\n\n    # Cap at 500 sites per chunk to avoid memory issues\n    chunk_size = min(chunk_size, 500)\n\n    # Don't exceed actual site count\n    return min(chunk_size, n_sites)\n\n\ndef chunk_items(items: Any, chunk_size: int) -> Iterator[List]:\n    \"\"\"Yield chunks of items from an iterable.\n\n    Args:\n        items: Iterable of items to chunk (list, generator, etc.)\n        chunk_size: Maximum items per chunk\n\n    Yields:\n        Lists of up to chunk_size items\n    \"\"\"\n    chunk = []\n    for item in items:\n        chunk.append(item)\n        if len(chunk) >= chunk_size:\n            yield chunk\n            chunk = []\n    if chunk:\n        yield chunk"
  },
  {
   "cell_type": "markdown",
   "id": "58539ce9",
   "metadata": {},
   "source": [
    "## ChunkedResult\n",
    "\n",
    "A result container that tracks both successful extractions and any errors that occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f7fc1d",
   "metadata": {},
   "outputs": [],
   "source": "#| export\n@dataclass\nclass ChunkedResult:\n    \"\"\"Result from chunked extraction with error tracking.\n\n    Attributes:\n        data: DataFrame containing successful extractions\n        errors: List of dicts with site_id, chunk_idx, and error message\n    \"\"\"\n    data: pd.DataFrame\n    errors: List[dict] = field(default_factory=list)\n\n    @property\n    def success_rate(self) -> float:\n        \"\"\"Fraction of sites successfully extracted.\"\"\"\n        n_success = len(self.data['site_id'].unique()) if len(self.data) > 0 else 0\n        n_errors = len(self.errors)\n        total = n_success + n_errors\n        return n_success / total if total > 0 else 1.0\n\n    def __repr__(self) -> str:\n        n_sites = len(self.data['site_id'].unique()) if len(self.data) > 0 else 0\n        return f\"ChunkedResult(sites={n_sites}, errors={len(self.errors)}, success_rate={self.success_rate:.1%})\""
  },
  {
   "cell_type": "markdown",
   "id": "77533975",
   "metadata": {},
   "source": [
    "## SiteCollection\n",
    "\n",
    "The `SiteCollection` class enables batch operations on multiple sites. It supports:\n",
    "\n",
    "- **Eager mode**: All Site objects created upfront (default for <1000 sites)\n",
    "- **Lazy mode**: Site objects created on-demand to save memory for large collections\n",
    "\n",
    "Use `SiteCollection` when you need to extract data from hundreds to thousands of sites efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6d81c7",
   "metadata": {},
   "outputs": [],
   "source": "#| export\nclass SiteCollection:\n    \"\"\"A collection of Sites for batch operations.\n\n    Supports two modes:\n    - Eager: All Site objects created upfront (default for <1000 sites)\n    - Lazy: Site objects created on-demand from stored feature dicts\n\n    Example:\n        # Load and extract from many sites\n        sites = SiteCollection.from_geojson('restoration_sites.geojson')\n\n        # Interactive batch extraction (Path A)\n        result = sites.extract_categorical(MAPBIOMAS_LULC, years=range(2010, 2024))\n        df = result.data\n\n        # Export for large collections (Path B)\n        task = sites.export_categorical(\n            MAPBIOMAS_LULC,\n            years=range(2010, 2024),\n            destination=ExportDestination(type='drive', folder='exports')\n        )\n    \"\"\"\n\n    def __init__(\n        self,\n        sites: Optional[List[Site]] = None,\n        feature_dicts: Optional[List[dict]] = None,\n        source_crs: str = 'EPSG:4326',\n        metadata: Optional[dict] = None\n    ):\n        \"\"\"Create a SiteCollection.\n\n        Args:\n            sites: List of Site objects (eager mode)\n            feature_dicts: List of GeoJSON feature dicts (lazy mode)\n            source_crs: CRS for feature_dicts (used in lazy mode)\n            metadata: Optional metadata dict (e.g., source file path)\n        \"\"\"\n        self._sites = sites\n        self._feature_dicts = feature_dicts\n        self._source_crs = source_crs\n        self._metadata = metadata or {}\n        self._fc_cache: Optional[ee.FeatureCollection] = None\n\n    def __len__(self) -> int:\n        if self._sites is not None:\n            return len(self._sites)\n        return len(self._feature_dicts) if self._feature_dicts else 0\n\n    def __iter__(self) -> Iterator[Site]:\n        if self._sites is not None:\n            yield from self._sites\n        elif self._feature_dicts:\n            for fd in self._feature_dicts:\n                yield Site.from_geojson(fd, source_crs=self._source_crs)\n\n    def __getitem__(self, idx: Union[int, slice]) -> Union[Site, 'SiteCollection']:\n        if isinstance(idx, int):\n            if self._sites is not None:\n                return self._sites[idx]\n            return Site.from_geojson(self._feature_dicts[idx], self._source_crs)\n        elif isinstance(idx, slice):\n            if self._sites is not None:\n                return SiteCollection(sites=self._sites[idx], metadata=self._metadata)\n            return SiteCollection(\n                feature_dicts=self._feature_dicts[idx],\n                source_crs=self._source_crs,\n                metadata=self._metadata\n            )\n        raise TypeError(f\"indices must be integers or slices, not {type(idx).__name__}\")\n\n    def __repr__(self) -> str:\n        mode = \"eager\" if self._sites is not None else \"lazy\"\n        return f\"SiteCollection(n={len(self)}, mode={mode})\"\n\n    @property\n    def site_ids(self) -> List[str]:\n        \"\"\"List of all site IDs in the collection.\"\"\"\n        return [site.site_id for site in self]\n\n    @property\n    def feature_collection(self) -> ee.FeatureCollection:\n        \"\"\"Get as ee.FeatureCollection (cached).\n\n        Note: This creates ee.Feature objects for all sites, which\n        may be slow for very large collections.\n        \"\"\"\n        if self._fc_cache is None:\n            features = [site.feature for site in self]\n            self._fc_cache = ee.FeatureCollection(features)\n        return self._fc_cache\n\n    def filter(self, predicate: Callable[[Site], bool]) -> 'SiteCollection':\n        \"\"\"Filter sites by a predicate function.\n\n        Args:\n            predicate: Function taking a Site, returning True to keep\n\n        Returns:\n            New SiteCollection with filtered sites\n        \"\"\"\n        filtered = [s for s in self if predicate(s)]\n        return SiteCollection(sites=filtered, metadata=self._metadata)\n\n    def filter_by_property(self, key: str, value: Any) -> 'SiteCollection':\n        \"\"\"Filter sites by a property value.\n\n        More efficient than filter() for lazy mode, as it operates\n        on feature dicts directly without creating Site objects.\n\n        Args:\n            key: Property name to filter on\n            value: Value to match\n\n        Returns:\n            New SiteCollection with matching sites\n        \"\"\"\n        if self._feature_dicts is not None and self._sites is None:\n            filtered = [\n                fd for fd in self._feature_dicts\n                if fd.get('properties', {}).get(key) == value\n            ]\n            return SiteCollection(\n                feature_dicts=filtered,\n                source_crs=self._source_crs,\n                metadata=self._metadata\n            )\n        return self.filter(lambda s: s.properties.get(key) == value)\n\n    @classmethod\n    def from_geojson(\n        cls,\n        path: Union[str, Path],\n        source_crs: Optional[str] = None,\n        lazy: bool = False,\n        lazy_threshold: int = 1000\n    ) -> 'SiteCollection':\n        \"\"\"Load sites from a GeoJSON file.\n\n        Args:\n            path: Path to GeoJSON file (FeatureCollection or single Feature)\n            source_crs: Override CRS (auto-detected from file if not provided)\n            lazy: Force lazy loading mode\n            lazy_threshold: Auto-switch to lazy if more features than this\n\n        Returns:\n            SiteCollection instance\n        \"\"\"\n        path = Path(path)\n\n        with open(path) as f:\n            data = json.load(f)\n\n        if source_crs is None:\n            source_crs = _detect_crs(data)\n\n        features = data.get('features', [data])\n\n        # Auto-switch to lazy for large files\n        if lazy or len(features) > lazy_threshold:\n            return cls(\n                feature_dicts=features,\n                source_crs=source_crs,\n                metadata={'path': str(path), 'mode': 'lazy'}\n            )\n        else:\n            sites = [Site.from_geojson(f, source_crs=source_crs) for f in features]\n            return cls(sites=sites, metadata={'path': str(path), 'mode': 'eager'})\n\n    @classmethod\n    def from_sites(cls, sites: List[Site], metadata: Optional[dict] = None) -> 'SiteCollection':\n        \"\"\"Create from a list of Site objects.\n\n        Args:\n            sites: List of Site instances\n            metadata: Optional metadata dict\n\n        Returns:\n            SiteCollection instance\n        \"\"\"\n        return cls(sites=sites, metadata=metadata)\n\n    @classmethod\n    def from_feature_collection(\n        cls,\n        fc: ee.FeatureCollection,\n        site_id_property: str = 'rid',\n        start_year_property: str = 'start_year'\n    ) -> 'SiteCollection':\n        \"\"\"Create from an ee.FeatureCollection.\n\n        Note: This triggers a getInfo() call to fetch feature data.\n        For large collections, prefer from_geojson() or export workflows.\n\n        Args:\n            fc: Earth Engine FeatureCollection\n            site_id_property: Property name for site ID\n            start_year_property: Property name for start year\n\n        Returns:\n            SiteCollection instance\n        \"\"\"\n        fc_info = fc.getInfo()\n        features = fc_info.get('features', [])\n\n        sites = []\n        for f in features:\n            props = f.get('properties', {})\n            geom = f.get('geometry', {})\n\n            # Reconstruct ee.Feature\n            geom_type = geom.get('type')\n            coords = geom.get('coordinates')\n\n            if geom_type == 'Polygon':\n                ee_geom = ee.Geometry.Polygon(coords)\n            elif geom_type == 'MultiPolygon':\n                ee_geom = ee.Geometry.MultiPolygon(coords)\n            else:\n                continue  # Skip unsupported types\n\n            ee_feature = ee.Feature(ee_geom, props)\n\n            site = Site(\n                ee_feature,\n                site_id=str(props.get(site_id_property, '')),\n                start_year=props.get(start_year_property)\n            )\n            sites.append(site)\n\n        return cls(sites=sites, metadata={'source': 'ee.FeatureCollection'})"
  },
  {
   "cell_type": "markdown",
   "id": "fabfe7ea",
   "metadata": {},
   "source": [
    "## Batch Extraction Methods (Path A)\n",
    "\n",
    "These methods extract data interactively, returning pandas DataFrames. Best for collections under ~5000 sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df3ba65",
   "metadata": {},
   "outputs": [],
   "source": "#| export\nfrom fastcore.basics import patch\n\n@patch\ndef extract_categorical(\n    self: SiteCollection,\n    layer: 'CategoricalLayer',\n    years: List[int],\n    chunk_size: Optional[int] = None,\n    max_pixels: int = int(1e9),\n    progress: bool = True\n) -> ChunkedResult:\n    \"\"\"Extract categorical data from all sites.\n\n    Batches sites into chunks to avoid GEE timeout/memory limits.\n    Each chunk is processed with a single getInfo() call.\n\n    Args:\n        layer: CategoricalLayer to extract from\n        years: List of years to extract\n        chunk_size: Sites per chunk (auto-calculated if None)\n        max_pixels: Max pixels per reduceRegion call\n        progress: Show progress bar (requires tqdm)\n\n    Returns:\n        ChunkedResult with DataFrame and any errors\n\n    Example:\n        result = sites.extract_categorical(MAPBIOMAS_LULC, years=[2020, 2021, 2022])\n        print(result)  # ChunkedResult(sites=500, errors=2, success_rate=99.6%)\n        df = result.data\n    \"\"\"\n    years_list = list(years)\n\n    if chunk_size is None:\n        chunk_size = calculate_chunk_size(len(years_list), len(self))\n\n    all_dfs = []\n    errors = []\n\n    # Set up progress bar\n    chunks = list(chunk_items(self, chunk_size))\n    if progress:\n        try:\n            from tqdm.auto import tqdm\n            chunks = tqdm(chunks, desc=\"Extracting categorical\")\n        except ImportError:\n            pass  # tqdm not available\n\n    for chunk_idx, chunk_sites in enumerate(chunks):\n        try:\n            chunk_df = _extract_categorical_chunk(\n                chunk_sites, layer, years_list, max_pixels\n            )\n            all_dfs.append(chunk_df)\n        except Exception as e:\n            # Record error for each site in the failed chunk\n            for site in chunk_sites:\n                errors.append({\n                    'site_id': site.site_id,\n                    'chunk_idx': chunk_idx,\n                    'error': str(e)\n                })\n\n    data = pd.concat(all_dfs, ignore_index=True) if all_dfs else pd.DataFrame()\n    return ChunkedResult(data=data, errors=errors)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24558f5",
   "metadata": {},
   "outputs": [],
   "source": "#| export\n@patch\ndef extract_continuous(\n    self: SiteCollection,\n    layer: 'ContinuousLayer',\n    start_date: str,\n    end_date: str,\n    reducer: str = 'mean',\n    frequency: str = 'yearly',\n    chunk_size: Optional[int] = None,\n    max_pixels: int = int(1e9),\n    progress: bool = True\n) -> ChunkedResult:\n    \"\"\"Extract continuous data from all sites.\n\n    Batches sites into chunks to avoid GEE timeout/memory limits.\n\n    Args:\n        layer: ContinuousLayer to extract from\n        start_date: Start date (YYYY-MM-DD)\n        end_date: End date (YYYY-MM-DD)\n        reducer: Spatial reducer ('mean', 'median', 'min', 'max')\n        frequency: Temporal grouping ('all', 'monthly', 'yearly')\n        chunk_size: Sites per chunk (auto-calculated if None)\n        max_pixels: Max pixels per reduceRegion call\n        progress: Show progress bar\n\n    Returns:\n        ChunkedResult with DataFrame and any errors\n    \"\"\"\n    # Estimate number of temporal periods for chunk size calculation\n    start_year = int(start_date[:4])\n    end_year = int(end_date[:4])\n    n_periods = end_year - start_year + 1\n    if frequency == 'monthly':\n        n_periods *= 12\n\n    if chunk_size is None:\n        chunk_size = calculate_chunk_size(n_periods, len(self))\n\n    all_dfs = []\n    errors = []\n\n    chunks = list(chunk_items(self, chunk_size))\n    if progress:\n        try:\n            from tqdm.auto import tqdm\n            chunks = tqdm(chunks, desc=\"Extracting continuous\")\n        except ImportError:\n            pass\n\n    for chunk_idx, chunk_sites in enumerate(chunks):\n        try:\n            chunk_df = _extract_continuous_chunk(\n                chunk_sites, layer, start_date, end_date,\n                reducer, frequency, max_pixels\n            )\n            all_dfs.append(chunk_df)\n        except Exception as e:\n            for site in chunk_sites:\n                errors.append({\n                    'site_id': site.site_id,\n                    'chunk_idx': chunk_idx,\n                    'error': str(e)\n                })\n\n    data = pd.concat(all_dfs, ignore_index=True) if all_dfs else pd.DataFrame()\n    return ChunkedResult(data=data, errors=errors)"
  },
  {
   "cell_type": "markdown",
   "id": "c0120f93",
   "metadata": {},
   "source": [
    "## Export Methods (Path B)\n",
    "\n",
    "These methods export data to Google Drive or Cloud Storage using GEE batch tasks. Best for large collections (>5000 sites)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ecd519",
   "metadata": {},
   "outputs": [],
   "source": "#| export\n@patch\ndef export_categorical(\n    self: SiteCollection,\n    layer: 'CategoricalLayer',\n    years: List[int],\n    destination: 'ExportDestination',\n    config: Optional['ExportConfig'] = None,\n    max_pixels: int = int(1e9)\n) -> 'ExportTask':\n    \"\"\"Export categorical extraction to Google Drive or Cloud Storage.\n\n    For collections larger than ~5000 sites, this is more reliable\n    than interactive extraction. Results are exported as CSV or GeoJSON\n    files, one per chunk.\n\n    Args:\n        layer: CategoricalLayer to extract from\n        years: List of years to extract\n        destination: Where to export (Drive or GCS)\n        config: Export configuration (chunk size, concurrency)\n        max_pixels: Max pixels per reduceRegion call\n\n    Returns:\n        ExportTask for monitoring progress\n\n    Example:\n        from gee_polygons.export import ExportDestination, ExportConfig\n\n        task = sites.export_categorical(\n            layer=MAPBIOMAS_LULC,\n            years=range(2010, 2024),\n            destination=ExportDestination(type='drive', folder='exports'),\n            config=ExportConfig(chunk_size=50, max_concurrent=15)\n        )\n\n        # Monitor progress\n        print(task.status())\n\n        # Wait for completion\n        task.wait(timeout_minutes=180)\n\n        # Get result file locations\n        print(task.results_info())\n    \"\"\"\n    from gee_polygons.export import ExportDestination, ExportConfig, ExportTask, _wait_for_task_slot\n\n    config = config or ExportConfig()\n    years_list = list(years)\n\n    task_ids = []\n    chunk_mapping = {}\n    active_tasks = []\n\n    chunks = list(chunk_items(self, config.chunk_size))\n\n    for chunk_idx, chunk_sites in enumerate(chunks):\n        # Wait if at max concurrent tasks\n        active_tasks = _wait_for_task_slot(active_tasks, config.max_concurrent)\n\n        # Build server-side FeatureCollection for this chunk\n        fc = _build_categorical_export_fc(chunk_sites, layer, years_list, max_pixels)\n\n        # Create export task\n        description = f\"{config.description_prefix}_cat_chunk_{chunk_idx:04d}\"\n        file_name = f\"{destination.file_prefix}_chunk_{chunk_idx:04d}\"\n\n        if destination.type == 'drive':\n            task = ee.batch.Export.table.toDrive(\n                collection=fc,\n                description=description,\n                folder=destination.folder,\n                fileNamePrefix=file_name,\n                fileFormat=destination.file_format\n            )\n        else:  # cloud_storage\n            bucket = destination.folder.split('/')[0]\n            prefix = '/'.join(destination.folder.split('/')[1:] + [file_name])\n            task = ee.batch.Export.table.toCloudStorage(\n                collection=fc,\n                description=description,\n                bucket=bucket,\n                fileNamePrefix=prefix,\n                fileFormat=destination.file_format\n            )\n\n        task.start()\n        task_id = task.id\n        task_ids.append(task_id)\n        active_tasks.append(task_id)\n\n        start_idx = chunk_idx * config.chunk_size\n        end_idx = start_idx + len(chunk_sites)\n        chunk_mapping[task_id] = (start_idx, end_idx)\n\n    return ExportTask(\n        task_ids=task_ids,\n        destination=destination,\n        config=config,\n        chunk_mapping=chunk_mapping\n    )"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3c5fb2",
   "metadata": {},
   "outputs": [],
   "source": "#| export\n@patch\ndef export_continuous(\n    self: SiteCollection,\n    layer: 'ContinuousLayer',\n    start_date: str,\n    end_date: str,\n    destination: 'ExportDestination',\n    reducer: str = 'mean',\n    frequency: str = 'yearly',\n    config: Optional['ExportConfig'] = None,\n    max_pixels: int = int(1e9)\n) -> 'ExportTask':\n    \"\"\"Export continuous extraction to Google Drive or Cloud Storage.\n\n    Args:\n        layer: ContinuousLayer to extract from\n        start_date: Start date (YYYY-MM-DD)\n        end_date: End date (YYYY-MM-DD)\n        destination: Where to export (Drive or GCS)\n        reducer: Spatial reducer ('mean', 'median', 'min', 'max')\n        frequency: Temporal grouping ('monthly', 'yearly')\n        config: Export configuration\n        max_pixels: Max pixels per reduceRegion call\n\n    Returns:\n        ExportTask for monitoring progress\n    \"\"\"\n    from gee_polygons.export import ExportDestination, ExportConfig, ExportTask, _wait_for_task_slot\n\n    if frequency == 'all':\n        raise ValueError(\"frequency='all' not supported for export (too many records)\")\n\n    config = config or ExportConfig()\n\n    task_ids = []\n    chunk_mapping = {}\n    active_tasks = []\n\n    chunks = list(chunk_items(self, config.chunk_size))\n\n    for chunk_idx, chunk_sites in enumerate(chunks):\n        active_tasks = _wait_for_task_slot(active_tasks, config.max_concurrent)\n\n        # Build server-side FeatureCollection\n        fc = _build_continuous_export_fc(\n            chunk_sites, layer, start_date, end_date,\n            reducer, frequency, max_pixels\n        )\n\n        description = f\"{config.description_prefix}_cont_chunk_{chunk_idx:04d}\"\n        file_name = f\"{destination.file_prefix}_chunk_{chunk_idx:04d}\"\n\n        if destination.type == 'drive':\n            task = ee.batch.Export.table.toDrive(\n                collection=fc,\n                description=description,\n                folder=destination.folder,\n                fileNamePrefix=file_name,\n                fileFormat=destination.file_format\n            )\n        else:\n            bucket = destination.folder.split('/')[0]\n            prefix = '/'.join(destination.folder.split('/')[1:] + [file_name])\n            task = ee.batch.Export.table.toCloudStorage(\n                collection=fc,\n                description=description,\n                bucket=bucket,\n                fileNamePrefix=prefix,\n                fileFormat=destination.file_format\n            )\n\n        task.start()\n        task_id = task.id\n        task_ids.append(task_id)\n        active_tasks.append(task_id)\n\n        start_idx = chunk_idx * config.chunk_size\n        end_idx = start_idx + len(chunk_sites)\n        chunk_mapping[task_id] = (start_idx, end_idx)\n\n    return ExportTask(\n        task_ids=task_ids,\n        destination=destination,\n        config=config,\n        chunk_mapping=chunk_mapping\n    )"
  },
  {
   "cell_type": "markdown",
   "id": "350d26cc",
   "metadata": {},
   "source": [
    "## Internal Extraction Functions\n",
    "\n",
    "These functions handle the actual GEE operations for each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228c255f",
   "metadata": {},
   "outputs": [],
   "source": "#| export\ndef _extract_categorical_chunk(\n    sites: List[Site],\n    layer: 'CategoricalLayer',\n    years: List[int],\n    max_pixels: int\n) -> pd.DataFrame:\n    \"\"\"Extract categorical data for a chunk of sites.\n\n    Builds a single ee.FeatureCollection containing all site-year combinations,\n    then calls getInfo() once for the entire chunk.\n    \"\"\"\n    records = []\n\n    for site in sites:\n        for year in years:\n            if layer.temporal_mode == 'band':\n                img = ee.Image(layer.asset_id)\n                band = layer.band_name(year)\n                classified = img.select(band)\n                band_name = band\n            else:\n                collection = (\n                    ee.ImageCollection(layer.asset_id)\n                    .filterDate(f'{year}-01-01', f'{year}-12-31')\n                    .filterBounds(site.geometry)\n                    .select(layer.band)\n                )\n                classified = collection.reduce(ee.Reducer.mode())\n                band_name = f'{layer.band}_mode'\n\n            stats = classified.reduceRegion(\n                reducer=ee.Reducer.frequencyHistogram(),\n                geometry=site.geometry,\n                scale=layer.scale,\n                maxPixels=max_pixels\n            )\n\n            hist = ee.Dictionary(stats.get(band_name))\n\n            records.append(\n                ee.Feature(None, {\n                    'site_id': site.site_id,\n                    'year': year,\n                    'histogram': hist\n                })\n            )\n\n    # Single getInfo call for entire chunk\n    fc = ee.FeatureCollection(records).getInfo()\n\n    # Convert to tidy rows\n    rows = []\n    for f in fc['features']:\n        props = f['properties']\n        site_id = props['site_id']\n        year = props['year']\n        hist = props['histogram']\n\n        if hist is None:\n            continue\n\n        for cls_str, count in hist.items():\n            cls = int(float(cls_str))\n            rows.append({\n                'site_id': site_id,\n                'year': year,\n                'class_value': cls,\n                'count': count,\n                'area_ha': count * (layer.scale ** 2) / 10_000,\n                'class_name': layer.class_name(cls)\n            })\n\n    return pd.DataFrame(rows)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8882a4",
   "metadata": {},
   "outputs": [],
   "source": "#| export\ndef _extract_continuous_chunk(\n    sites: List[Site],\n    layer: 'ContinuousLayer',\n    start_date: str,\n    end_date: str,\n    reducer: str,\n    frequency: str,\n    max_pixels: int\n) -> pd.DataFrame:\n    \"\"\"Extract continuous data for a chunk of sites.\n\n    Builds a single ee.FeatureCollection for all site-period combinations.\n    \"\"\"\n    reducer_fn = getattr(ee.Reducer, reducer)()\n    bands = layer.bands\n\n    # Build temporal periods\n    if frequency == 'yearly':\n        periods = _build_yearly_periods(start_date, end_date)\n    elif frequency == 'monthly':\n        periods = _build_monthly_periods(start_date, end_date)\n    else:\n        # 'all' frequency - handled differently below\n        periods = None\n\n    records = []\n\n    for site in sites:\n        # Load and preprocess collection for this site\n        collection = (\n            ee.ImageCollection(layer.collection_id)\n            .filterDate(start_date, end_date)\n            .filterBounds(site.geometry)\n        )\n\n        if layer.preprocess is not None:\n            collection = collection.map(layer.preprocess)\n\n        collection = collection.select(bands)\n\n        if frequency == 'all':\n            # One record per image\n            def reduce_image(img):\n                stats = img.reduceRegion(\n                    reducer=reducer_fn,\n                    geometry=site.geometry,\n                    scale=layer.scale,\n                    maxPixels=max_pixels\n                )\n                props = {\n                    'site_id': site.site_id,\n                    'date': img.date().format('YYYY-MM-dd')\n                }\n                for band in bands:\n                    props[band] = stats.get(band)\n                return ee.Feature(None, props)\n\n            site_records = collection.map(reduce_image)\n            records.append(site_records)\n\n        else:\n            # Aggregate by period\n            for period in periods:\n                filtered = collection.filterDate(period['start'], period['end'])\n                composite = filtered.median()\n\n                stats = composite.reduceRegion(\n                    reducer=reducer_fn,\n                    geometry=site.geometry,\n                    scale=layer.scale,\n                    maxPixels=max_pixels\n                )\n\n                props = {'site_id': site.site_id}\n                props.update(period['props'])\n                for band in bands:\n                    props[band] = stats.get(band)\n\n                records.append(ee.Feature(None, props))\n\n    # Handle 'all' frequency differently (records are FeatureCollections)\n    if frequency == 'all':\n        merged = ee.FeatureCollection([])\n        for fc in records:\n            merged = merged.merge(fc)\n        fc = merged.getInfo()\n    else:\n        fc = ee.FeatureCollection(records).getInfo()\n\n    # Convert to rows\n    rows = []\n    for f in fc['features']:\n        props = f['properties']\n\n        # Skip if all band values are None\n        if all(props.get(b) is None for b in bands):\n            continue\n\n        row = {'site_id': props['site_id']}\n\n        if frequency == 'all':\n            row['date'] = props.get('date')\n        elif frequency == 'yearly':\n            row['year'] = props.get('year')\n        elif frequency == 'monthly':\n            row['year'] = props.get('year')\n            row['month'] = props.get('month')\n\n        for band in bands:\n            row[band] = props.get(band)\n\n        rows.append(row)\n\n    return pd.DataFrame(rows)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c867461b",
   "metadata": {},
   "outputs": [],
   "source": "#| export\ndef _build_yearly_periods(start_date: str, end_date: str) -> List[dict]:\n    \"\"\"Build list of yearly periods.\"\"\"\n    start_year = int(start_date[:4])\n    end_year = int(end_date[:4])\n\n    periods = []\n    for year in range(start_year, end_year + 1):\n        periods.append({\n            'start': f'{year}-01-01',\n            'end': f'{year}-12-31',\n            'props': {'year': year}\n        })\n    return periods\n\n\ndef _build_monthly_periods(start_date: str, end_date: str) -> List[dict]:\n    \"\"\"Build list of monthly periods.\"\"\"\n    import datetime\n\n    start = datetime.date.fromisoformat(start_date)\n    end = datetime.date.fromisoformat(end_date)\n\n    periods = []\n    current = start.replace(day=1)\n\n    while current <= end:\n        year, month = current.year, current.month\n        if month == 12:\n            next_month = datetime.date(year + 1, 1, 1)\n        else:\n            next_month = datetime.date(year, month + 1, 1)\n\n        periods.append({\n            'start': current.isoformat(),\n            'end': next_month.isoformat(),\n            'props': {'year': year, 'month': month}\n        })\n\n        current = next_month\n\n    return periods"
  },
  {
   "cell_type": "markdown",
   "id": "6ea52bee",
   "metadata": {},
   "source": [
    "## Export Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb4fea7",
   "metadata": {},
   "outputs": [],
   "source": "#| export\ndef _build_categorical_export_fc(\n    sites: List[Site],\n    layer: 'CategoricalLayer',\n    years: List[int],\n    max_pixels: int\n) -> ee.FeatureCollection:\n    \"\"\"Build FeatureCollection for categorical export.\n\n    Creates a flattened structure where each feature represents one\n    site-year-class combination with columns:\n    site_id, year, class_value, count, area_ha, class_name\n    \"\"\"\n    records = []\n\n    for site in sites:\n        for year in years:\n            if layer.temporal_mode == 'band':\n                img = ee.Image(layer.asset_id)\n                band = layer.band_name(year)\n                classified = img.select(band)\n                band_name = band\n            else:\n                collection = (\n                    ee.ImageCollection(layer.asset_id)\n                    .filterDate(f'{year}-01-01', f'{year}-12-31')\n                    .filterBounds(site.geometry)\n                    .select(layer.band)\n                )\n                classified = collection.reduce(ee.Reducer.mode())\n                band_name = f'{layer.band}_mode'\n\n            stats = classified.reduceRegion(\n                reducer=ee.Reducer.frequencyHistogram(),\n                geometry=site.geometry,\n                scale=layer.scale,\n                maxPixels=max_pixels\n            )\n\n            hist = ee.Dictionary(stats.get(band_name))\n\n            # Store histogram as JSON string for export\n            # (GEE export doesn't handle nested dicts well)\n            records.append(\n                ee.Feature(None, {\n                    'site_id': site.site_id,\n                    'year': year,\n                    'histogram': hist,\n                    'scale': layer.scale\n                })\n            )\n\n    return ee.FeatureCollection(records)\n\n\ndef _build_continuous_export_fc(\n    sites: List[Site],\n    layer: 'ContinuousLayer',\n    start_date: str,\n    end_date: str,\n    reducer: str,\n    frequency: str,\n    max_pixels: int\n) -> ee.FeatureCollection:\n    \"\"\"Build FeatureCollection for continuous export.\"\"\"\n    reducer_fn = getattr(ee.Reducer, reducer)()\n    bands = layer.bands\n\n    if frequency == 'yearly':\n        periods = _build_yearly_periods(start_date, end_date)\n    elif frequency == 'monthly':\n        periods = _build_monthly_periods(start_date, end_date)\n    else:\n        raise ValueError(f\"Unsupported frequency for export: {frequency}\")\n\n    records = []\n\n    for site in sites:\n        collection = (\n            ee.ImageCollection(layer.collection_id)\n            .filterDate(start_date, end_date)\n            .filterBounds(site.geometry)\n        )\n\n        if layer.preprocess is not None:\n            collection = collection.map(layer.preprocess)\n\n        collection = collection.select(bands)\n\n        for period in periods:\n            filtered = collection.filterDate(period['start'], period['end'])\n            composite = filtered.median()\n\n            stats = composite.reduceRegion(\n                reducer=reducer_fn,\n                geometry=site.geometry,\n                scale=layer.scale,\n                maxPixels=max_pixels\n            )\n\n            props = {'site_id': site.site_id}\n            props.update(period['props'])\n            for band in bands:\n                props[band] = stats.get(band)\n\n            records.append(ee.Feature(None, props))\n\n    return ee.FeatureCollection(records)"
  },
  {
   "cell_type": "markdown",
   "id": "4c0d8dae",
   "metadata": {},
   "source": [
    "## Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736d6c36",
   "metadata": {},
   "outputs": [],
   "source": "#| eval: false\n# Initialize Earth Engine\nee.Authenticate()\nee.Initialize(project=\"your-project-id\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd25281",
   "metadata": {},
   "outputs": [],
   "source": "#| eval: false\n# Load a collection of sites\nsites = SiteCollection.from_geojson('../data/restoration_sites_subset.geojson')\nprint(sites)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a2a023",
   "metadata": {},
   "outputs": [],
   "source": "#| eval: false\n# Extract categorical data (Path A - interactive)\nfrom gee_polygons.datasets.mapbiomas import MAPBIOMAS_LULC\n\nresult = sites.extract_categorical(MAPBIOMAS_LULC, years=range(2018, 2023))\nprint(result)\nresult.data.head()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b84a6f",
   "metadata": {},
   "outputs": [],
   "source": "#| hide\nimport nbdev; nbdev.nbdev_export()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
